# ğŸ§  MÃ³dulo 7: Multimodalidad con IA

> **SesiÃ³n 17** â€” Curso de Inteligencia Artificial Aplicada  
> Repositorio de materiales, ejercicios y ejemplos prÃ¡cticos sobre modelos multimodales.

---

## ğŸ“Œ DescripciÃ³n

Este mÃ³dulo explora el concepto de **multimodalidad** en los modelos de inteligencia artificial: la capacidad de procesar y generar informaciÃ³n en mÃºltiples formatos, combinando texto, imÃ¡genes, audio y video en un mismo flujo de trabajo.

A lo largo de este mÃ³dulo aprenderÃ¡s a:

- Comprender quÃ© son los modelos multimodales y cÃ³mo funcionan
- Trabajar con APIs de visiÃ³n artificial (anÃ¡lisis de imÃ¡genes con LLMs)
- Procesar audio y transcribir voz a texto
- Generar imÃ¡genes a partir de descripciones textuales
- Integrar mÃºltiples modalidades en aplicaciones reales

---

## ğŸ—‚ï¸ Estructura del Repositorio

```
Modulo7_Multimodalidad/
â”œâ”€â”€ notebooks/           # Jupyter Notebooks con ejercicios y ejemplos
â”œâ”€â”€ scripts/             # Scripts Python listos para ejecutar
â”œâ”€â”€ assets/              # ImÃ¡genes y archivos de ejemplo
â”œâ”€â”€ slides/              # Presentaciones de la sesiÃ³n
â””â”€â”€ README.md
```

---

## ğŸš€ TecnologÃ­as Utilizadas

| TecnologÃ­a | Uso |
|---|---|
| **OpenAI GPT-4o / Vision** | AnÃ¡lisis y descripciÃ³n de imÃ¡genes |
| **Whisper** | TranscripciÃ³n de audio a texto |
| **DALL-E / Stable Diffusion** | GeneraciÃ³n de imÃ¡genes desde texto |
| **Claude (Anthropic)** | Modelos multimodales alternativos |
| **Python 3.10+** | Lenguaje base de los ejemplos |
| **Jupyter Notebooks** | Entorno interactivo de aprendizaje |

---

## âš™ï¸ Requisitos e InstalaciÃ³n

### Prerrequisitos

- Python 3.10 o superior
- Cuenta en [OpenAI](https://platform.openai.com/) (para GPT-4o y Whisper)
- Cuenta en [Anthropic](https://console.anthropic.com/) *(opcional)*

### InstalaciÃ³n

```bash
# 1. Clona el repositorio
git clone https://github.com/alzamoralabs/Modulo7_Multimodalidad.git
cd Modulo7_Multimodalidad

# 2. Crea un entorno virtual
python -m venv venv
source venv/bin/activate       # Linux/macOS
# venv\Scripts\activate        # Windows

# 3. Instala las dependencias
pip install -r requirements.txt
```

### Variables de entorno

Crea un archivo `.env` en la raÃ­z del proyecto con tus claves de API:

```env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

---

## ğŸ“š Contenidos del MÃ³dulo

### ğŸ” 1. IntroducciÃ³n a la Multimodalidad
- Â¿QuÃ© es un modelo multimodal?
- Diferencias entre modelos unimodales y multimodales
- Casos de uso reales

### ğŸ–¼ï¸ 2. VisiÃ³n por Computadora con LLMs
- AnÃ¡lisis de imÃ¡genes con GPT-4o Vision
- ExtracciÃ³n de informaciÃ³n de documentos e imÃ¡genes
- OCR inteligente con modelos de lenguaje

### ğŸ™ï¸ 3. Procesamiento de Audio
- TranscripciÃ³n de audio con Whisper
- AnÃ¡lisis de sentimiento en voz
- Speech-to-text en tiempo real

### ğŸ¨  4. GeneraciÃ³n de ImÃ¡genes
- Texto a imagen con DALL-E 3
- Prompting avanzado para generaciÃ³n visual
- EdiciÃ³n y variaciÃ³n de imÃ¡genes

### ğŸ”— 5. IntegraciÃ³n Multimodal
- Pipelines que combinan texto + imagen + audio
- ConstrucciÃ³n de aplicaciones multimodales
- Casos prÃ¡cticos y proyectos finales

---

## ğŸ§ª Ejemplos RÃ¡pidos

### AnÃ¡lisis de imagen con GPT-4o

```python
from openai import OpenAI
import base64

client = OpenAI()

with open("imagen.jpg", "rb") as f:
    imagen_b64 = base64.b64encode(f.read()).decode("utf-8")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Â¿QuÃ© ves en esta imagen?"},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{imagen_b64}"}}
            ]
        }
    ]
)

print(response.choices[0].message.content)
```

### TranscripciÃ³n de audio con Whisper

```python
from openai import OpenAI

client = OpenAI()

with open("audio.mp3", "rb") as audio_file:
    transcripcion = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="es"
    )

print(transcripcion.text)
```

---

## ğŸ“– Recursos Adicionales

- [DocumentaciÃ³n de OpenAI](https://platform.openai.com/docs)
- [DocumentaciÃ³n de Anthropic](https://docs.anthropic.com)
- [Whisper en GitHub](https://github.com/openai/whisper)
- [Paper: GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

---

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Si encuentras un error o tienes alguna mejora:

1. Haz un fork del repositorio
2. Crea una rama: `git checkout -b feature/mi-mejora`
3. Realiza tus cambios y haz commit: `git commit -m 'Agrega nueva funcionalidad'`
4. Sube los cambios: `git push origin feature/mi-mejora`
5. Abre un Pull Request

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia **MIT**. Consulta el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ‘¤ Autor

**alzamoralabs**  
ğŸ”— [GitHub](https://github.com/alzamoralabs)

---

<p align="center">Hecho con â¤ï¸ para el aprendizaje de IA</p>